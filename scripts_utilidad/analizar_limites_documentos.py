#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Script para analizar el tama√±o de documentos procesados y l√≠mites del sistema
"""

import os
import sys
from dotenv import load_dotenv

# Cargar variables de entorno
load_dotenv()

# Agregar path de Firebase
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'cliente_web'))

try:
    from firebase_config import db
except Exception as e:
    print(f"‚ùå Error importando Firebase: {e}")
    sys.exit(1)

def analizar_documentos():
    """Analiza todos los documentos procesados en Firestore"""
    
    print("=" * 80)
    print("üìä AN√ÅLISIS DE DOCUMENTOS PROCESADOS")
    print("=" * 80)
    
    try:
        # Obtener todos los documentos procesados
        docs = db.collection('documentos').where('estado', '==', 'procesado').stream()
        
        documentos = []
        for doc in docs:
            data = doc.to_dict()
            contenido = data.get('contenidoProcesado', '')
            
            documentos.append({
                'id': doc.id,
                'nombre': data.get('nombre', 'Sin nombre'),
                'usuario': data.get('usuarioId', 'Unknown'),
                'tama√±o': len(contenido),
                'palabras': len(contenido.split()) if contenido else 0,
                'lineas': contenido.count('\n') if contenido else 0
            })
        
        if not documentos:
            print("‚ùå No hay documentos procesados en el sistema")
            return
        
        # Ordenar por tama√±o
        documentos.sort(key=lambda x: x['tama√±o'], reverse=True)
        
        print(f"\nüìö Total de documentos procesados: {len(documentos)}\n")
        
        # Estad√≠sticas generales
        total_chars = sum(d['tama√±o'] for d in documentos)
        total_words = sum(d['palabras'] for d in documentos)
        
        print("üìä ESTAD√çSTICAS GENERALES")
        print("-" * 80)
        print(f"Total caracteres: {total_chars:,}")
        print(f"Total palabras: {total_words:,}")
        print(f"Promedio chars/doc: {total_chars // len(documentos):,}")
        print(f"Promedio palabras/doc: {total_words // len(documentos):,}")
        
        # Documentos m√°s grandes
        print("\nüìÑ TOP 10 DOCUMENTOS M√ÅS GRANDES")
        print("-" * 80)
        print(f"{'Nombre':<50} {'Caracteres':>15} {'Palabras':>12}")
        print("-" * 80)
        
        for doc in documentos[:10]:
            nombre = doc['nombre'][:47] + '...' if len(doc['nombre']) > 50 else doc['nombre']
            print(f"{nombre:<50} {doc['tama√±o']:>15,} {doc['palabras']:>12,}")
        
        # An√°lisis de l√≠mites
        print("\n‚ö†Ô∏è  AN√ÅLISIS DE L√çMITES")
        print("-" * 80)
        
        # L√≠mites del sistema
        LIMITE_CONTEXTO_ACTUAL = 60000  # chars (configuraci√≥n actual)
        LIMITE_DEEPSEEK_TOKENS = 64000  # tokens (aprox. 256k chars)
        CHARS_POR_TOKEN_ESTIMADO = 4
        LIMITE_DEEPSEEK_CHARS = LIMITE_DEEPSEEK_TOKENS * CHARS_POR_TOKEN_ESTIMADO
        
        print(f"üîß L√≠mite actual del sistema: {LIMITE_CONTEXTO_ACTUAL:,} caracteres")
        print(f"ü§ñ L√≠mite de DeepSeek (estimado): ~{LIMITE_DEEPSEEK_CHARS:,} caracteres")
        print(f"   (64k tokens √ó ~{CHARS_POR_TOKEN_ESTIMADO} chars/token)")
        
        # Documentos que exceden el l√≠mite
        docs_exceden = [d for d in documentos if d['tama√±o'] > LIMITE_CONTEXTO_ACTUAL]
        
        if docs_exceden:
            print(f"\n‚ö†Ô∏è  {len(docs_exceden)} documentos exceden el l√≠mite actual:")
            for doc in docs_exceden[:5]:
                exceso = doc['tama√±o'] - LIMITE_CONTEXTO_ACTUAL
                porcentaje = (exceso / doc['tama√±o']) * 100
                print(f"   ‚Ä¢ {doc['nombre'][:60]}")
                print(f"     Tama√±o: {doc['tama√±o']:,} chars | Exceso: {exceso:,} chars ({porcentaje:.1f}%)")
        
        # Recomendaciones
        print("\nüí° RECOMENDACIONES")
        print("-" * 80)
        
        doc_mas_grande = documentos[0]
        
        if doc_mas_grande['tama√±o'] > LIMITE_DEEPSEEK_CHARS:
            print("üî¥ CR√çTICO: Documento m√°s grande excede l√≠mite de DeepSeek")
            print(f"   Documento: {doc_mas_grande['nombre']}")
            print(f"   Tama√±o: {doc_mas_grande['tama√±o']:,} caracteres")
            print(f"   Acci√≥n: Implementar chunking o dividir documento")
        elif doc_mas_grande['tama√±o'] > LIMITE_CONTEXTO_ACTUAL:
            print("üü° ADVERTENCIA: Documento m√°s grande excede l√≠mite configurado")
            print(f"   Documento: {doc_mas_grande['nombre']}")
            print(f"   Tama√±o: {doc_mas_grande['tama√±o']:,} caracteres")
            print(f"   Acci√≥n: ‚úÖ Ya implementado - b√∫squeda inteligente activa")
        else:
            print("‚úÖ Todos los documentos dentro de l√≠mites manejables")
        
        print("\nüîç B√öSQUEDA INTELIGENTE (Implementada)")
        print("-" * 80)
        print("‚úì Extrae secciones relevantes bas√°ndose en palabras clave")
        print("‚úì M√°ximo 60,000 caracteres de contexto por consulta")
        print("‚úì Prioriza p√°rrafos con coincidencias de la pregunta")
        print("‚úì Mantiene coherencia del contenido")
        
        # Estimaci√≥n de tokens para DeepSeek
        print("\nüìä ESTIMACI√ìN DE USO DE TOKENS")
        print("-" * 80)
        
        for doc in documentos[:5]:
            tokens_estimados = doc['tama√±o'] // CHARS_POR_TOKEN_ESTIMADO
            print(f"{doc['nombre'][:60]}")
            print(f"  Caracteres: {doc['tama√±o']:,} | Tokens estimados: ~{tokens_estimados:,}")
        
        print("\n" + "=" * 80)
        
    except Exception as e:
        print(f"‚ùå Error analizando documentos: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    analizar_documentos()
